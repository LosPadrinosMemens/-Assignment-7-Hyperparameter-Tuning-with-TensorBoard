{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you will need:\n",
    "- a GPU\n",
    "- To have installed the corresponding versions of CUDA Toolkit and cuDNN library to your GPU.\n",
    "\n",
    "1. After that you need to create a new environment in anaconda as follows:\n",
    "```\n",
    "    conda create -n tf_gpu python=3.8 \n",
    "```\n",
    "2. Install the corresponding packages via the anaconda prompt:\n",
    "```\n",
    "    conda activate tf_gpu\n",
    "    conda install -c anaconda tensorflow-gpu keras-gpu\n",
    "    conda install numpy=1.18.5\n",
    "    conda install cudatoolkit=10.1\n",
    "```\n",
    "3. Configure the Jupyter kernel\n",
    "```\n",
    "python -m ipykernel install --user --name tf_gpu --display-name \"Python (GPU)\"\n",
    "```\n",
    "4. Install packages\n",
    "``` python\n",
    "pip install Pillow\n",
    "pip install tensorflow==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# %load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assesing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUST print TRUE\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 665669926521770458\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12598196140159282792\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3028759348\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17604719507136690646\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15578366525513547133\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# MUST LIST YOU GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE IS A BUG HERE WHERE YOU CANNOT TURN OFF THE LOG DEVICE PLACEMENT, SO UNCOMMENT THIS LINES CHECK THAT IT IS USING YOU GPU< THEN RESTART THE KERNEL AND COMMENT THESE LINES\n",
    "# MUST USE YOUR GPU FOR MATRIX MULTIPLICATION\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "# \n",
    "# c = tf.matmul(a, b)\n",
    "# \n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using the same dataset as the previous assignments in which the problem is to classify a set of images as cats or not retrieved from https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification?select=train. We removed the picture \"dog_505.png\" as it caused problems while preprocessing it. We will first process only 100 images per class (100 cats and 100 dogs for training and another 100 cats and 100 dogs for validation) to reduce their pixel resolution to the same as the other assignments (64 pixels x 64 pixels) and then represent them as arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    dataset_flatten = dataset.reshape(dataset.shape[0],-1).T\n",
    "    return dataset_flatten/255 \n",
    "\n",
    "def process_images(directory, target_size=(64, 64), image_range=range(0,100)):\n",
    "    image_list = []\n",
    "\n",
    "    filenames = sorted(os.listdir(directory))[min(image_range):max(image_range)+1]\n",
    "    \n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            img = Image.open(file_path)\n",
    "            img_resized = img.resize(target_size)\n",
    "            img_array = np.array(img_resized)\n",
    "            \n",
    "            image_list.append(img_array)\n",
    "    \n",
    "    return np.array(image_list)\n",
    "\n",
    "def join_cats_and_dogs(cat_images,dog_images):\n",
    "    cat_set_X = preprocess_dataset(cat_images)\n",
    "    dog_set_X = preprocess_dataset(dog_images)\n",
    "    m_cat_set = cat_set_X.shape[1]\n",
    "    cat_set_Y = np.ones((1, m_cat_set))\n",
    "    m_dog_set = dog_set_X.shape[1]\n",
    "    dog_set_Y = np.zeros((1, m_dog_set))\n",
    "\n",
    "    set_X = np.concatenate((cat_set_X, dog_set_X), axis=1)\n",
    "    set_Y = np.concatenate((cat_set_Y, dog_set_Y), axis=1)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    shuffle_indices = np.random.permutation(set_X.shape[1])\n",
    "    final_set_X = set_X[:, shuffle_indices]\n",
    "    final_set_Y = set_Y[:, shuffle_indices]\n",
    "\n",
    "    return final_set_X,final_set_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_images = process_images(\"datasets/train/cats\")\n",
    "train_dog_images = process_images(\"datasets/train/dogs\")\n",
    "\n",
    "test_cat_images = process_images(\"datasets/train/cats\",image_range=(101,200))\n",
    "test_dog_images = process_images(\"datasets/train/dogs\",image_range=(101,200))\n",
    "\n",
    "x_train, y_train = join_cats_and_dogs(train_cat_images,train_dog_images)\n",
    "y_train = y_train.flatten()\n",
    "x_train = x_train.T\n",
    "\n",
    "\n",
    "x_test, y_test = join_cats_and_dogs(test_cat_images,test_dog_images)\n",
    "y_test = y_test.flatten()\n",
    "x_test = x_test.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "log_dir = r'logs\\hparam_tuning'\n",
    "shutil.rmtree(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "We will tune 4 parameters: the number of units, the dropout frequency, the batch size and the optimizer used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.05, 0.10, 0.15, 0.20]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([16, 32]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     # Add a Reshape layer to unflatten the input\n",
    "#     tf.keras.layers.Reshape((64, 64, 3), input_shape=(12288,)),\n",
    "#     \n",
    "#     # Block 1\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "#     \n",
    "#     # Block 2\n",
    "#     tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "#     tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "#     \n",
    "#     # Flatten the output of the conv layers to feed into the dense layers\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     \n",
    "#     # Fully connected layers with dropout in between\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "#     tf.keras.layers.Dense(4096, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "#     \n",
    "#     # Output layer\n",
    "#     tf.keras.layers.Dense(2, activation='softmax'),  # Adjust the number of units for the number of classes\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  model = tf.keras.models.Sequential([\n",
    "        # Reshape layer to unflatten the input\n",
    "        tf.keras.layers.Reshape((64, 64, 3), input_shape=(12288,)),\n",
    "        \n",
    "        # Block 1 - Simplified to one Conv2D layer\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Block 2 - Removed entirely or you can add another with fewer filters if necessary\n",
    "        # tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        # tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "        \n",
    "        # Flatten the output of the conv layers to feed into the dense layers\n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        # Fully connected layers with dropout in between and reduced size\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),  # Reduced from 4096 to 1024\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        \n",
    "        # Output layer - adjust the number of units for the number of classes\n",
    "        tf.keras.layers.Dense(2, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  callbacks = [\n",
    "  keras.callbacks.TensorBoard(\n",
    "  log_dir=log_dir,\n",
    "  histogram_freq=1,\n",
    "  embeddings_freq=1,\n",
    "  )\n",
    "  ]\n",
    "\n",
    "  model.fit(x_train, y_train, epochs=1,callbacks=callbacks, batch_size=hparams[HP_BATCH_SIZE]) \n",
    "  _, accuracy = model.evaluate(x_test, y_test)\n",
    "  return accuracy\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    full_run_dir = os.path.join('logs', 'hparam_tuning', run_dir)\n",
    "    ensure_dir(full_run_dir)  # Ensure the directory exists\n",
    "    with tf.summary.create_file_writer(full_run_dir).as_default():\n",
    "        hp.hparams(hparams)  # Record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs\\\\hparam_tuning'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'dropout': 0.05, 'optimizer': 'adam', 'batch_size': 16}\n",
      " 2/13 [===>..........................] - ETA: 10s - loss: 27.9336 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 1.7173s). Check your callbacks.\n",
      "12/13 [==========================>...] - ETA: 0s - loss: 10.2453 - accuracy: 0.4688"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a NewWriteableFile: logs\\hparam_tuning\\train\\keras_embedding.ckpt-0_temp_631780fe02b643f1b935e85f9a6fdc6a/part-00000-of-00001.data-00000-of-00001.tempstate737473315795254130 : The system cannot find the path specified.\r\n; No such process [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--- Starting trial: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m run_name)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[1;32m---> 14\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[54], line 53\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_dir, hparams)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(full_run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m     52\u001b[0m     hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# Record the values used in this trial\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mscalar(METRIC_ACCURACY, accuracy, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[54], line 39\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[1;34m(hparams)\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mhparams[HP_OPTIMIZER],\n\u001b[0;32m     27\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     32\u001b[0m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[0;32m     33\u001b[0m log_dir\u001b[38;5;241m=\u001b[39mlog_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m ]\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHP_BATCH_SIZE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     40\u001b[0m _, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1137\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1134\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1135\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1137\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:412\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    411\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 412\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m numpy_logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Only convert once.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2185\u001b[0m, in \u001b[0;36mTensorBoard.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2182\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_weights(epoch)\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_freq \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2185\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2266\u001b[0m, in \u001b[0;36mTensorBoard._log_embeddings\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m   2263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[0;32m   2264\u001b[0m   embeddings_ckpt \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_write_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2265\u001b[0m                                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras_embedding.ckpt-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m-> 2266\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_ckpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2101\u001b[0m, in \u001b[0;36mModel.save_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (optimizer\n\u001b[0;32m   2093\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, trackable\u001b[38;5;241m.\u001b[39mTrackable)):\n\u001b[0;32m   2094\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   2095\u001b[0m       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis model was compiled with a Keras optimizer (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) but is being \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2096\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved in TensorFlow format with `save_weights`. The model\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2099\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using a TensorFlow optimizer from `tf.train`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2100\u001b[0m       \u001b[38;5;241m%\u001b[39m (optimizer,))\n\u001b[1;32m-> 2101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;66;03m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m checkpoint_management\u001b[38;5;241m.\u001b[39mupdate_checkpoint_state_internal(\n\u001b[0;32m   2104\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(filepath),\n\u001b[0;32m   2105\u001b[0m     model_checkpoint_path\u001b[38;5;241m=\u001b[39mfilepath,\n\u001b[0;32m   2106\u001b[0m     save_relative_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2107\u001b[0m     all_model_checkpoint_paths\u001b[38;5;241m=\u001b[39m[filepath])\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1199\u001b[0m, in \u001b[0;36mTrackableSaver.save\u001b[1;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[0;32m   1196\u001b[0m   object_graph_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m file_io\u001b[38;5;241m.\u001b[39mrecursive_create_dir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_prefix))\n\u001b[1;32m-> 1199\u001b[0m save_path, new_feed_additions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_cached_when_graph_building\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_graph_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_feed_additions:\n\u001b[0;32m   1202\u001b[0m   feed_dict\u001b[38;5;241m.\u001b[39mupdate(new_feed_additions)\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1145\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_save_object_graph \u001b[38;5;241m!=\u001b[39m graph_proto\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;66;03m# When executing eagerly, we need to re-create SaveableObjects each time\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;66;03m# save() is called so they pick up new Tensors passed to their\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;66;03m# constructors. That means the Saver needs to be copied with a new\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;66;03m# var_list.\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function()):\n\u001b[0;32m   1144\u001b[0m   saver \u001b[38;5;241m=\u001b[39m functional_saver\u001b[38;5;241m.\u001b[39mMultiDeviceSaver(named_saveable_objects)\n\u001b[1;32m-> 1145\u001b[0m   save_op \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([save_op]):\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:295\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    293\u001b[0m   tf_function_save()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:269\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save.<locals>.save_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    264\u001b[0m   sharded_prefixes\u001b[38;5;241m.\u001b[39mappend(shard_prefix)\n\u001b[0;32m    265\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# _SingleDeviceSaver will use the CPU device when necessary, but\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# initial read operations should be placed on the SaveableObject's\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# device.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     sharded_saves\u001b[38;5;241m.\u001b[39mappend(\u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(sharded_saves):\n\u001b[0;32m    272\u001b[0m   \u001b[38;5;66;03m# Merge on the io_device if specified, otherwise co-locates the merge op\u001b[39;00m\n\u001b[0;32m    273\u001b[0m   \u001b[38;5;66;03m# with the last device used.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m   merge_device \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    275\u001b[0m       options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    276\u001b[0m       saveable_object_util\u001b[38;5;241m.\u001b[39mset_cpu0(last_device))\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:78\u001b[0m, in \u001b[0;36m_SingleDeviceSaver.save\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m     76\u001b[0m save_device \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(save_device):\n\u001b[1;32m---> 78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:1729\u001b[0m, in \u001b[0;36msave_v2\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1729\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_v2_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_and_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m   1733\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:1750\u001b[0m, in \u001b[0;36msave_v2_eager_fallback\u001b[1;34m(prefix, tensor_names, shape_and_slices, tensors, name, ctx)\u001b[0m\n\u001b[0;32m   1748\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [prefix, tensor_names, shape_and_slices] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(tensors)\n\u001b[0;32m   1749\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_dtypes)\n\u001b[1;32m-> 1750\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSaveV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "File \u001b[1;32mc:\\Users\\13042\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a NewWriteableFile: logs\\hparam_tuning\\train\\keras_embedding.ckpt-0_temp_631780fe02b643f1b935e85f9a6fdc6a/part-00000-of-00001.data-00000-of-00001.tempstate737473315795254130 : The system cannot find the path specified.\r\n; No such process [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for dropout_rate in HP_DROPOUT.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "        for batch_size in HP_BATCH_SIZE.domain.values:  # Add this loop\n",
    "            hparams = {\n",
    "                HP_DROPOUT: dropout_rate,\n",
    "                HP_OPTIMIZER: optimizer,\n",
    "                HP_BATCH_SIZE: batch_size,  # Include batch size\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run(log_dir + run_name, hparams)\n",
    "            session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10265223786447933628\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13037884004019447277\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10747635510916229690\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2355745228207664348,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11784309633994895745\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 8026317427330174693\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Using GPU for computation\n",
      "tf.Tensor(\n",
      "[[1. 3.]\n",
      " [3. 7.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "\n",
    "# Run a matrix multiplication operation and log the device that it's executed on.\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6016 (pid 29612), started 0:00:14 ago. (Use '!kill 29612' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dff165af2e6a7f7b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dff165af2e6a7f7b\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6016;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
